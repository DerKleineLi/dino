{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vision_transformer as vits\n",
    "import utils\n",
    "from vision_transformer import DINOHead\n",
    "from main_dino import DINOLoss\n",
    "from vot.models.factory import (\n",
    "    create_model,\n",
    "    create_optimizer,\n",
    "    dino_load_checkpoint,\n",
    "    dino_save_checkpoint,\n",
    "    MultiCropWrapper as VotMCW,\n",
    "    PatchEmbeddingConv,\n",
    ")\n",
    "import MinkowskiEngine as ME\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino = vits.__dict__[\"vit_small\"](patch_size=16)\n",
    "dino_head = DINOHead(dino.embed_dim, 65536, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/hli/dl_ws/voxel-transformer/configs/2d_dino.yaml\", \"r\") as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "teacher = create_model(config[\"model\"])\n",
    "vot = teacher.backbone\n",
    "vot_head = teacher.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    res = 0\n",
    "    for p in model.parameters():\n",
    "        res += p.numel()\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0361, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_head.last_layer.weight.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0361, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vot_head.last_layer.weight.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22352128"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(dino_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22352128"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(vot_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 384])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino.cls_token.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 384])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vot.class_token.class_position_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "with torch.no_grad():\n",
    "    vot.patch_embedding.head_conv.kernel[:] = rearrange(\n",
    "        dino.patch_embed.proj.weight, \"p c w h -> (h w) c p\"\n",
    "    )\n",
    "    vot.patch_embedding.head_conv.bias[:] = dino.patch_embed.proj.bias\n",
    "\n",
    "    vot.position_embedding.position_embedding[:] = dino.pos_embed[0, 1:]\n",
    "    vot.class_token.class_position_embedding[:] = dino.pos_embed[0, 0]\n",
    "    vot.class_token.class_embedding[:] = dino.cls_token\n",
    "\n",
    "    vot.encoder.blocks.load_state_dict(dino.blocks.state_dict())\n",
    "    vot.encoder.encoder_norm.load_state_dict(dino.norm.state_dict())\n",
    "\n",
    "    vot_head.load_state_dict(dino_head.state_dict())\n",
    "\n",
    "    # vot.patch_embedding = dino.patch_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment.hybrid import Hybrid\n",
    "\n",
    "hybrid_model = Hybrid(dino, vot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "sinput = ME.to_sparse(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(vot.patch_embedding, PatchEmbeddingConv):\n",
    "    patch_embeddings, patch_ids, sparse_patch_embeddings = vot.patch_embedding(sinput)\n",
    "else:\n",
    "    patch_embeddings = vot.patch_embedding(input)\n",
    "    B, P, C = patch_embeddings.shape\n",
    "    patch_ids = torch.arange(P).view(1, -1).expand(B, -1)\n",
    "    sparse_patch_embeddings = None\n",
    "patch_embeddings[0, torch.where(patch_ids == 0)[1]]\n",
    "\n",
    "patch_embeddings = patch_embeddings + vot.position_embedding(patch_ids)\n",
    "pos_embeddings = vot.position_embedding(patch_ids)\n",
    "\n",
    "patch_embeddings, masks = vot.class_token(patch_embeddings, patch_ids < 0)\n",
    "patch_embeddings[0, -1]\n",
    "\n",
    "block = vot.encoder.blocks[0]\n",
    "attn_layer = block.attn\n",
    "\n",
    "B, N, C = patch_embeddings.shape\n",
    "qkv = (\n",
    "    attn_layer.qkv(patch_embeddings)\n",
    "    .reshape(B, N, 3, attn_layer.heads, C // attn_layer.heads)\n",
    "    .permute(2, 0, 3, 1, 4)\n",
    ")\n",
    "q, k, v = (\n",
    "    qkv[0],\n",
    "    qkv[1],\n",
    "    qkv[2],\n",
    ")\n",
    "\n",
    "attn = (q @ k.transpose(-2, -1)) * attn_layer.scale\n",
    "# if mask is not None:\n",
    "#     attn = attn.masked_fill(mask.view(B, 1, 1, -1), float(\"-inf\"))\n",
    "# attn = attn.softmax(dim=-1)\n",
    "# attn = self.attn_drop(attn)\n",
    "\n",
    "# x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "# x = self.proj(x)\n",
    "# x = self.proj_drop(x)\n",
    "\n",
    "# x = x + self.drop_path(y)\n",
    "# x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "# patch_embeddings, hidden_states = vot.encoder(patch_embeddings, masks)\n",
    "# hidden_states[-1][0, -1]\n",
    "# patch_embeddings[0, -1]\n",
    "\n",
    "# vot_out = vot_head(patch_embeddings[:, -1])\n",
    "# vot_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, patch_emb, pos_emb = dino.prepare_tokens(input)\n",
    "patch_emb[0, 0]\n",
    "x[0, 0]\n",
    "\n",
    "block = dino.blocks[0]\n",
    "attn_layer = block.attn\n",
    "\n",
    "B, N, C = x.shape\n",
    "qkv_dino = (\n",
    "    attn_layer.qkv(x)\n",
    "    .reshape(B, N, 3, attn_layer.num_heads, C // attn_layer.num_heads)\n",
    "    .permute(2, 0, 3, 1, 4)\n",
    ")\n",
    "q_dino, k_dino, v_dino = qkv_dino[0], qkv_dino[1], qkv_dino[2]\n",
    "\n",
    "attn_dino = (q_dino @ k_dino.transpose(-2, -1)) * attn_layer.scale\n",
    "\n",
    "\n",
    "# attn = attn.softmax(dim=-1)\n",
    "# attn = self.attn_drop(attn)\n",
    "\n",
    "# x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "# x = self.proj(x)\n",
    "# x = self.proj_drop(x)\n",
    "\n",
    "# y_dino, attn = block.attn(block.norm1(x))\n",
    "# y_dino\n",
    "\n",
    "# x = x + self.drop_path(y)\n",
    "# x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "# hidden_states_dino = []\n",
    "# for blk in dino.blocks:\n",
    "#     x = blk(x)\n",
    "#     hidden_states_dino.append(x)\n",
    "# x[0, 0]\n",
    "\n",
    "# x = dino.norm(x)\n",
    "# x[0, 0]\n",
    "\n",
    "# dino_out = dino_head(x[:, 0])\n",
    "# dino_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, patch_emb, pos_emb = hybrid_model.dino.prepare_tokens(input)\n",
    "masks = torch.zeros(x.size(0), x.size(1), dtype=bool)\n",
    "# x[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_emb, hidden_states, sparse_patch_emb = vot(sinput,debug=True)\n",
    "# output, patch_emb, pos_emb, hidden_states = dino(input,debug=True)\n",
    "vot_out = vot(sinput)\n",
    "dino_out = dino(input)\n",
    "hybrid_out = hybrid_model(input)\n",
    "# vot.patch_embedding = dino.patch_embed\n",
    "# vot_out_2 = vot(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2352e-06, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dino_out - vot_out).abs().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2352e-06, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dino_out - hybrid_out).abs().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_teacher = utils.MultiCropWrapper(\n",
    "    dino,\n",
    "    dino_head,\n",
    ")\n",
    "vot_teacher = VotMCW(vot, vot_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [torch.randn(1, 3, 224, 224) for _ in range(10)]\n",
    "sinput = [ME.to_sparse(i) for i in input]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(dino_teacher) == get_num_params(vot_teacher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dino_out \u001b[39m=\u001b[39m dino_teacher(\u001b[39minput\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m vot_out \u001b[39m=\u001b[39m vot_teacher(sinput)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dl_ws/voxel-transformer/vot/models/dino.py:86\u001b[0m, in \u001b[0;36mMultiCropWrapper.forward\u001b[0;34m(self, x, backbone_only)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     85\u001b[0m     x \u001b[39m=\u001b[39m [x]\n\u001b[0;32m---> 86\u001b[0m x \u001b[39m=\u001b[39m inference_one_list(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone, x)\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m backbone_only:\n\u001b[1;32m     88\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/dl_ws/voxel-transformer/vot/models/dino.py:14\u001b[0m, in \u001b[0;36minference_one_list\u001b[0;34m(model, inputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference_one_list\u001b[39m(model, inputs):\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([model(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs])\n",
      "File \u001b[0;32m~/dl_ws/voxel-transformer/vot/models/dino.py:14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minference_one_list\u001b[39m(model, inputs):\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat([model(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs])\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dl_ws/voxel-transformer/vot/models/vot.py:86\u001b[0m, in \u001b[0;36mPatchAttendedBackbone.forward\u001b[0;34m(self, sparse_input, class_token_only, debug)\u001b[0m\n\u001b[1;32m     82\u001b[0m     patch_embeddings, patch_ids, sparse_patch_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch_embedding(\n\u001b[1;32m     83\u001b[0m         sparse_input\n\u001b[1;32m     84\u001b[0m     )\n\u001b[1;32m     85\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     patch_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatch_embedding(sparse_input)\n\u001b[1;32m     87\u001b[0m     B, P, C \u001b[39m=\u001b[39m patch_embeddings\u001b[39m.\u001b[39mshape\n\u001b[1;32m     88\u001b[0m     patch_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(P)\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mexpand(B, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/dl_ws/dino/vision_transformer.py:174\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 174\u001b[0m     B, C, H, W \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\n\u001b[1;32m    175\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x)\u001b[39m.\u001b[39mflatten(\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    176\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "dino_out = dino_teacher(input)\n",
    "vot_out = vot_teacher(sinput)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.4703e-08, -5.9605e-08,  3.1665e-08,  ...,  4.8429e-08,\n",
       "         0.0000e+00, -7.0781e-08], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_out[2] - vot_out[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hli/miniconda3/envs/dl-py39/lib/python3.9/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from main_dino import DataAugmentationDINO\n",
    "from torchvision import datasets\n",
    "\n",
    "transform = DataAugmentationDINO(\n",
    "    (0.4, 1.0),\n",
    "    (0.05, 0.4),\n",
    "    8,\n",
    ")\n",
    "data_path = \"/mnt/raid/hli/datasets/imagenet/ilsvrc2012/train\"\n",
    "dataset = datasets.ImageFolder(data_path, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(images, _) = next(iter(data_loader))\n",
    "simages = [ME.to_sparse(i) for i in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_out = dino_teacher(images)\n",
    "vot_out = vot_teacher(simages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.4703e-07, grad_fn=<MinBackward1>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dino_out - vot_out).min()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8c93ec7d301fbc7af32f2f0f6a4ce273ba8be80dd27f98a13b93c30d55f32a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
