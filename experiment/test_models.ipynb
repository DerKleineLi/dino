{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vision_transformer as vits\n",
    "import utils\n",
    "from vision_transformer import DINOHead\n",
    "from main_dino import DINOLoss\n",
    "from vot.models.factory import (\n",
    "    create_model,\n",
    "    create_optimizer,\n",
    "    dino_load_checkpoint,\n",
    "    dino_save_checkpoint,\n",
    "    MultiCropWrapper as VotMCW,\n",
    "    PatchEmbeddingConv\n",
    ")\n",
    "import MinkowskiEngine as ME\n",
    "import yaml\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino = vits.__dict__[\"vit_small\"](patch_size=16)\n",
    "dino2 = vits.__dict__[\"vit_small\"](patch_size=16)\n",
    "dino_head = DINOHead(dino.embed_dim, 65536, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/hli/dl_ws/voxel-transformer/configs/2d_dino.yaml\", \"r\") as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "vot_student = create_model(config[\"model\"])\n",
    "vot_teacher = create_model(config[\"model\"])\n",
    "vot = vot_student.backbone\n",
    "vot_head = vot_student.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(model):\n",
    "    res = 0\n",
    "    for p in model.parameters():\n",
    "        res += p.numel()\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "\n",
    "with torch.no_grad():\n",
    "    vot.patch_embedding.head_conv.kernel[:] = rearrange(dino.patch_embed.proj.weight, \"p c w h -> (h w) c p\")\n",
    "    vot.patch_embedding.head_conv.bias[:] = dino.patch_embed.proj.bias\n",
    "    \n",
    "    vot.position_embedding.position_embedding[:] = dino.pos_embed[0,1:]\n",
    "    vot.class_token.class_position_embedding[:] = dino.pos_embed[0,0]\n",
    "    vot.class_token.class_embedding[:] = dino.cls_token\n",
    "    \n",
    "    vot.encoder.blocks.load_state_dict(dino.blocks.state_dict())\n",
    "    vot.encoder.encoder_norm.load_state_dict(dino.norm.state_dict())\n",
    "    \n",
    "    vot_head.load_state_dict(dino_head.state_dict())\n",
    "    \n",
    "    # vot.patch_embedding = dino.patch_embed\n",
    "    # vot_teacher.backbone.patch_embedding = dino2.patch_embed\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_student = utils.MultiCropWrapper(\n",
    "    dino,\n",
    "    dino_head,\n",
    ")\n",
    "dino_t = vits.__dict__[\"vit_small\"](patch_size=16)\n",
    "dino_head_t = DINOHead(dino.embed_dim, 65536, False)\n",
    "dino_teacher = utils.MultiCropWrapper(\n",
    "    dino_t,\n",
    "    dino_head_t,\n",
    ")\n",
    "\n",
    "dino_teacher.load_state_dict(dino_student.state_dict())\n",
    "vot_teacher.load_state_dict(vot_student.state_dict())\n",
    "for p in dino_teacher.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in vot_teacher.parameters():\n",
    "    p.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hli/miniconda3/envs/dl-py39/lib/python3.9/site-packages/torchvision/transforms/transforms.py:891: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from main_dino import DataAugmentationDINO\n",
    "from torchvision import datasets\n",
    "\n",
    "transform = DataAugmentationDINO(\n",
    "    (0.4, 1.0),\n",
    "    (0.05, 0.4),\n",
    "    8,\n",
    ")\n",
    "data_path = \"/mnt/raid/hli/datasets/imagenet/ilsvrc2012/train\"\n",
    "dataset = datasets.ImageFolder(data_path, transform=transform)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,\n",
    "    num_workers=6,\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "(images, _) = next(iter(data_loader))\n",
    "simages = [ME.to_sparse(i) for i in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_out = dino_student(images)\n",
    "vot_out = vot_student(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dino_out - vot_out).abs().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_out_teacher = dino_teacher(images[:2])\n",
    "vot_out_teacher = vot_teacher(images[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dino_out_teacher - vot_out_teacher).abs().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3411e-07, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dino_out[: dino_out_teacher.size(0)] - dino_out_teacher).abs().max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3411e-07, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vot_out[: dino_out_teacher.size(0)] - vot_out_teacher).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_loss = create_model(config[\"loss\"])\n",
    "dino_loss_vot = create_model(config[\"loss\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dino_loss = DINOLoss(\n",
    "#     65536,\n",
    "#     10,  # total number of crops = 2 global crops + local_crops_number\n",
    "#     0.04,\n",
    "#     0.04,\n",
    "#     0,\n",
    "#     100,\n",
    "# )\n",
    "\n",
    "# dino_loss_vot = DINOLoss(\n",
    "#     65536,\n",
    "#     10,  # total number of crops = 2 global crops + local_crops_number\n",
    "#     0.04,\n",
    "#     0.04,\n",
    "#     0,\n",
    "#     100,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dino = dino_loss(dino_out, dino_out_teacher, 0)\n",
    "loss_vot = dino_loss_vot(vot_out, vot_out_teacher, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_groups = utils.get_params_groups(dino_student)\n",
    "dino_optimizer = torch.optim.AdamW(dino_student.parameters(), lr=2e-2)\n",
    "params_groups = utils.get_params_groups(vot_student)\n",
    "vot_optimizer = torch.optim.AdamW(vot_student.parameters(), lr=2e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_optimizer.zero_grad()\n",
    "vot_optimizer.zero_grad()\n",
    "loss_dino.backward()\n",
    "loss_vot.backward()\n",
    "dino_optimizer.step()\n",
    "vot_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     m = 0.996\n",
    "#     for param_q, param_k in zip(\n",
    "#         vot_student.module.parameters(), vot_teacher.parameters()\n",
    "#     ):\n",
    "#         param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
    "#     for param_q, param_k in zip(\n",
    "#         dino_student.module.parameters(), dino_teacher.parameters()\n",
    "#     ):\n",
    "#         param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0406, -0.0160, -0.0508, -0.0788, -0.0277,  0.0516,  0.0071,  0.0134,\n",
       "        -0.0638,  0.0393,  0.0709,  0.0069, -0.0521, -0.0743,  0.0380,  0.0685,\n",
       "        -0.0932,  0.0174,  0.0303,  0.0507, -0.0652, -0.0673, -0.0420,  0.0147,\n",
       "         0.0627, -0.0602,  0.0704,  0.0489, -0.0409, -0.0781,  0.0263, -0.0107,\n",
       "         0.0243,  0.0628, -0.0437,  0.0511, -0.0152,  0.0177,  0.0448, -0.0543,\n",
       "        -0.0644, -0.0366, -0.0384,  0.0656, -0.0806,  0.0610, -0.0516, -0.0928,\n",
       "         0.0188, -0.0287,  0.0320,  0.0562, -0.0133,  0.0606,  0.0432,  0.0212,\n",
       "         0.0243,  0.0748,  0.0561, -0.0433,  0.0810,  0.0507, -0.0499,  0.0503,\n",
       "        -0.0699,  0.0397,  0.0663,  0.0137, -0.0827, -0.0110,  0.0114, -0.1011,\n",
       "         0.0384, -0.0285,  0.0589, -0.0755, -0.0154, -0.0271,  0.0640, -0.0073,\n",
       "        -0.0058,  0.0028, -0.0493,  0.0092, -0.0685, -0.0549, -0.0716, -0.0608,\n",
       "        -0.0339, -0.0545, -0.0264,  0.0459,  0.0831, -0.0317,  0.0181,  0.0473,\n",
       "        -0.0142,  0.0448,  0.0282, -0.0545,  0.0152,  0.0244, -0.0238,  0.0829,\n",
       "        -0.0113, -0.0300, -0.0552, -0.0679, -0.0387, -0.0037, -0.0640, -0.0794,\n",
       "         0.0341,  0.0113,  0.0580, -0.0237,  0.0515,  0.0233,  0.1103,  0.0729,\n",
       "        -0.0550, -0.0724, -0.0297,  0.0502, -0.0281,  0.0494, -0.0348,  0.0697,\n",
       "         0.0185, -0.0424,  0.0818, -0.0777,  0.0757, -0.0178,  0.0549, -0.0413,\n",
       "        -0.0448, -0.0575, -0.0335, -0.0089, -0.0246, -0.0221,  0.0488,  0.0124,\n",
       "         0.0002,  0.0224, -0.0525, -0.1270, -0.0018, -0.0537,  0.0541, -0.0870,\n",
       "         0.0833,  0.0199,  0.0009, -0.0597,  0.0289, -0.0277, -0.0057, -0.0746,\n",
       "        -0.0058,  0.0027, -0.0314,  0.0535,  0.0859, -0.0664,  0.0398,  0.0346,\n",
       "         0.0745, -0.0796, -0.0602,  0.0363,  0.0332, -0.0064,  0.0629,  0.0254,\n",
       "        -0.0675, -0.0627,  0.0909, -0.0584, -0.0346,  0.0566,  0.0040,  0.0531,\n",
       "         0.0433, -0.0132,  0.0632,  0.0384, -0.0583,  0.0327,  0.0697,  0.0501,\n",
       "        -0.0316, -0.0635,  0.0808,  0.0692, -0.0858,  0.0241, -0.0802, -0.0370,\n",
       "         0.0214, -0.0662,  0.0087, -0.0676,  0.0032, -0.0297, -0.0462, -0.0450,\n",
       "         0.0444, -0.1159,  0.0058,  0.0305,  0.0456, -0.0420,  0.0845,  0.0576,\n",
       "        -0.0616, -0.0668, -0.0622, -0.0462,  0.0067,  0.0732,  0.0559,  0.0356,\n",
       "         0.0453,  0.0088, -0.0476,  0.0330,  0.0724, -0.0474, -0.0415, -0.0373,\n",
       "        -0.0191, -0.0257, -0.0405,  0.0353, -0.0956,  0.0432,  0.0244, -0.0795,\n",
       "         0.0506,  0.0558, -0.0078,  0.0284, -0.0162,  0.0543,  0.0304,  0.0526,\n",
       "         0.0076, -0.0815,  0.0436, -0.0822, -0.0198, -0.0747,  0.0295,  0.0199,\n",
       "         0.0539, -0.0313,  0.0735,  0.0207, -0.0288,  0.0066, -0.0505, -0.0460,\n",
       "        -0.0556, -0.0795,  0.0375, -0.0195,  0.0562, -0.0359,  0.0315, -0.0009,\n",
       "        -0.0081, -0.0375,  0.0177, -0.0176,  0.0450, -0.0644, -0.0039, -0.0006,\n",
       "        -0.0555,  0.0581, -0.0606, -0.0491, -0.0578,  0.0296, -0.0512,  0.0441,\n",
       "        -0.0263, -0.1216, -0.0238, -0.0461, -0.0549,  0.0641, -0.0784, -0.0412,\n",
       "         0.0579,  0.0048,  0.0055, -0.0072,  0.0100,  0.0381,  0.0694, -0.0434,\n",
       "         0.0079,  0.0388,  0.0698, -0.0529, -0.0455, -0.0044,  0.0455, -0.0671,\n",
       "         0.0504, -0.0701, -0.0388, -0.0111, -0.0829,  0.0241,  0.0272,  0.0687,\n",
       "         0.0464, -0.0853, -0.0293,  0.0330, -0.1091, -0.0546, -0.0539, -0.0246,\n",
       "         0.0202, -0.0240, -0.0383, -0.0302, -0.0377,  0.0234, -0.0473,  0.0219,\n",
       "         0.0727,  0.0596,  0.0063, -0.0586,  0.0643,  0.0680, -0.0639, -0.0804,\n",
       "        -0.0368,  0.0745, -0.0675, -0.0217,  0.0615, -0.0145, -0.0260,  0.0578,\n",
       "         0.0289, -0.0660,  0.0463, -0.0438,  0.0363,  0.0096,  0.0592, -0.0555,\n",
       "        -0.0698,  0.0077, -0.0557,  0.0532,  0.0569, -0.0672, -0.0047,  0.0206,\n",
       "        -0.0494, -0.0457,  0.0629, -0.0213,  0.0288,  0.0377,  0.0135, -0.0143,\n",
       "        -0.0076,  0.0685,  0.0138,  0.0491, -0.0711, -0.0186,  0.0435,  0.0610],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if isinstance(vot.patch_embedding, PatchEmbeddingConv):\n",
    "    patch_embeddings, patch_ids, sparse_patch_embeddings = vot.patch_embedding(\n",
    "        sinput\n",
    "    )\n",
    "else:\n",
    "    patch_embeddings = vot.patch_embedding(input)\n",
    "    B, P, C = patch_embeddings.shape\n",
    "    patch_ids = torch.arange(P).view(1, -1).expand(B, -1)\n",
    "    sparse_patch_embeddings = None\n",
    "patch_embeddings[0, torch.where(patch_ids == 0)[1][0]]\n",
    "\n",
    "patch_embeddings = patch_embeddings + vot.position_embedding(patch_ids)\n",
    "pos_embeddings = vot.position_embedding(patch_ids)\n",
    "\n",
    "patch_embeddings, masks = vot.class_token(patch_embeddings, patch_ids < 0)\n",
    "patch_embeddings[0, 0]\n",
    "\n",
    "# block = vot.encoder.blocks[0]\n",
    "# attn_layer = block.attn\n",
    "\n",
    "# B, N, C = patch_embeddings.shape\n",
    "# qkv = (\n",
    "#     attn_layer.qkv(patch_embeddings)\n",
    "#     .reshape(B, N, 3, attn_layer.heads, C // attn_layer.heads)\n",
    "#     .permute(2, 0, 3, 1, 4)\n",
    "# )\n",
    "# q, k, v = (\n",
    "#     qkv[0],\n",
    "#     qkv[1],\n",
    "#     qkv[2],\n",
    "# )\n",
    "\n",
    "# attn = (q @ k.transpose(-2, -1)) * attn_layer.scale\n",
    "# if mask is not None:\n",
    "#     attn = attn.masked_fill(mask.view(B, 1, 1, -1), float(\"-inf\"))\n",
    "# attn = attn.softmax(dim=-1)\n",
    "# attn = self.attn_drop(attn)\n",
    "\n",
    "# x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "# x = self.proj(x)\n",
    "# x = self.proj_drop(x)\n",
    "\n",
    "# x = x + self.drop_path(y)\n",
    "# x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "# patch_embeddings, hidden_states = vot.encoder(patch_embeddings, masks)\n",
    "# hidden_states[-1][0, -1]\n",
    "# patch_embeddings[0, -1]\n",
    "\n",
    "# vot_out = vot_head(patch_embeddings[:, -1])\n",
    "# vot_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-21.6251,  23.6143,  23.8853,  15.8956,  24.5549,  25.0263, -22.1332,\n",
       "         23.9854, -24.2066,  23.8880,  24.1964,  12.5992,   6.4231, -16.7931,\n",
       "         -6.6260, -15.9072, -23.9110,  20.5405,   5.2969,   9.2776, -23.5279,\n",
       "         15.5631,  23.1687, -24.6560,   1.4549,  11.8108, -24.4652,  -9.6431,\n",
       "        -23.9609,   9.0096,   4.4318, -24.5280, -24.6686,  24.3940, -24.0790,\n",
       "         11.9851,  -7.0437,  17.2459,  11.9501, -24.3065,  24.7551, -10.6594,\n",
       "         24.6832,  24.0830,  12.1820, -22.6551,  11.5486, -25.9461, -23.7650,\n",
       "         11.8115,   5.9917, -19.7801,  10.0713, -24.1953, -17.2989,  24.4109,\n",
       "         24.5908, -22.9933, -21.6915, -24.7751,  24.0575,  18.9463, -19.1986,\n",
       "        -23.5262,  25.0119,  23.7467, -12.4631,  25.0144, -24.8882,  17.6349,\n",
       "        -24.3782, -24.8354, -23.7090, -23.8478,  10.3990, -24.6180, -23.9910,\n",
       "        -24.6055,  24.6195,  24.4928, -11.3367, -23.7406, -24.7030, -19.7857,\n",
       "         23.9775,  24.8067,  23.8190, -16.6945,  12.2922,  25.2634,  -7.9049,\n",
       "         23.6041,  24.1284,   9.0991, -24.0441,  21.2939, -24.5348,   2.3634,\n",
       "        -22.1631,  24.2778, -24.8944,  24.3477,  24.3144,   7.0899, -24.4931,\n",
       "        -20.7893,   5.0375,   9.5128, -23.8406,  24.8706,  -8.1406, -24.3792,\n",
       "        -14.3008, -24.5773,  24.5249,   9.7510, -22.1743, -23.6500,  24.6960,\n",
       "         -6.1527,  23.4091, -25.3369, -24.1225, -19.3795,  23.8001,  24.6844,\n",
       "         23.7818, -23.9053,  22.3887,  22.9138, -11.5095,  11.5703,  23.1711,\n",
       "        -22.1206,  14.1004,  24.5027,  17.7136,  24.5858,  17.9360,  24.6155,\n",
       "         24.2837, -23.1073,  24.0928, -22.5291,  -7.2486, -23.8873,  12.9629,\n",
       "          4.7936, -13.7335,   6.8845,  23.5559,   2.9061, -24.9484,  24.0673,\n",
       "        -24.1066,  15.9342, -24.2904,  24.1697, -24.1657,  24.8592,  25.1810,\n",
       "          6.1053, -24.8959,  23.7917,  23.9494,  -7.1478,  -8.9697,  24.0574,\n",
       "         24.6978,  24.1601,   6.3017, -25.2030, -24.5054, -11.1935,  23.7704,\n",
       "         -7.4011,  10.2245,  23.4259, -23.2851, -24.6022,   9.7728,  24.4280,\n",
       "         12.1159,  23.6994, -24.1341, -24.3500, -23.8974, -12.4976, -10.2223,\n",
       "         23.5156,  11.8548, -18.8013,   7.8111, -23.5878,   3.2082, -23.6670,\n",
       "        -24.0070, -24.7897, -11.5995, -24.3501, -11.4509,  23.9358, -10.4594,\n",
       "         24.2153,  22.2890,  24.8009,  24.0553,  15.6451, -23.9424,  12.2860,\n",
       "         24.5937,  23.8690,   8.4008,  11.3402, -19.7136, -24.7205,  23.6125,\n",
       "        -24.2223, -24.5083, -25.2964,  14.7217, -24.7284,  12.4550,  -0.3645,\n",
       "        -24.5790, -24.2996,  13.9259,  23.1666,  20.6818, -24.3087,  11.2350,\n",
       "         19.5506,  10.4440,  24.1860,  -4.9527,  20.1988,  -2.9471,   0.6949,\n",
       "        -24.8753, -24.8282,  -0.2971,  25.0296, -23.6451, -20.4490, -24.4724,\n",
       "        -24.2230,  24.8476,  24.8549, -24.0039, -24.2658, -23.6895, -23.9243,\n",
       "         25.2576, -11.7747,  24.1150,  23.9817,  23.0743, -12.8212, -24.4624,\n",
       "        -10.6260, -23.8342, -10.0481,   0.7794,  -0.2681, -24.6057,   6.5661,\n",
       "          5.2628,   3.2324,  23.7420, -24.1309,   7.1162,   7.4658,  -4.8884,\n",
       "        -11.4957, -24.5684,  21.5233, -24.6622,  -2.2296,  24.6617, -24.5291,\n",
       "        -23.0557,   3.7201,  11.6284,  24.7328, -23.3147, -23.7596, -20.8686,\n",
       "         23.7530,  16.0580,  11.8390, -25.1176,  11.8888,  11.2641, -25.0022,\n",
       "         24.4341,  -5.4997, -15.1563,  23.8063,  23.3757,   4.7535, -24.4497,\n",
       "        -25.3427, -24.7521,  24.5117, -12.9596,  20.0855,  25.2532,  23.4624,\n",
       "         25.5615,  24.1727, -21.4292, -11.6839,  23.9193, -23.2567, -25.1497,\n",
       "        -10.4827,  11.7896,  24.4693, -24.0996,  24.4866,  -3.0481,   6.3932,\n",
       "        -20.2306, -25.0094, -14.3425, -25.0171, -12.4748, -12.0227,   8.9197,\n",
       "         25.4050,   8.2767, -24.5733,  10.9306,  -1.3380, -20.4118,   5.6920,\n",
       "         24.0442,  24.5339, -24.0353,   2.4632,   6.7328, -24.5818,  22.7319,\n",
       "        -23.3325, -24.1911,  -7.3386, -23.0387, -24.9945, -24.2400, -15.2133,\n",
       "         24.0498,  -6.7332, -23.9772, -23.8052,   6.5621,  -7.3620,  23.8410,\n",
       "          0.7475,  23.8727, -23.5929, -24.1902, -25.0025, -23.8217, -23.6341,\n",
       "         24.2778, -24.5806,  23.2860, -13.0577,  10.5529,  -7.2870,  -1.7517,\n",
       "         -8.5234,  11.4256,  24.0407,  23.5156,  -5.7987, -24.1042,  24.4843,\n",
       "         25.2626,  23.7456,  11.0463, -23.7026,  24.5453,  -3.8880],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, patch_emb, pos_emb = dino.prepare_tokens(input)\n",
    "patch_emb[0, 0]\n",
    "# x[0, 0]\n",
    "\n",
    "# block = dino.blocks[0]\n",
    "# attn_layer = block.attn\n",
    "\n",
    "# B, N, C = x.shape\n",
    "# qkv_dino = (\n",
    "#     attn_layer.qkv(x)\n",
    "#     .reshape(B, N, 3, attn_layer.num_heads, C // attn_layer.num_heads)\n",
    "#     .permute(2, 0, 3, 1, 4)\n",
    "# )\n",
    "# q_dino, k_dino, v_dino = qkv_dino[0], qkv_dino[1], qkv_dino[2]\n",
    "\n",
    "# attn_dino = (q_dino @ k_dino.transpose(-2, -1)) * attn_layer.scale\n",
    "# attn = attn.softmax(dim=-1)\n",
    "# attn = self.attn_drop(attn)\n",
    "\n",
    "# x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "# x = self.proj(x)\n",
    "# x = self.proj_drop(x)\n",
    "        \n",
    "# y_dino, attn = block.attn(block.norm1(x))\n",
    "# y_dino\n",
    "\n",
    "# x = x + self.drop_path(y)\n",
    "# x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "# hidden_states_dino = []\n",
    "# for blk in dino.blocks:\n",
    "#     x = blk(x)\n",
    "#     hidden_states_dino.append(x)\n",
    "# x[0, 0]\n",
    "\n",
    "# x = dino.norm(x)\n",
    "# x[0, 0]\n",
    "\n",
    "# dino_out = dino_head(x[:, 0])\n",
    "# dino_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5027e-06, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vot.position_embedding.position_embedding - dino.pos_embed[0,1:]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7311e-11)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vot.position_embedding.position_embedding.grad - dino.pos_embed.grad[0,1:]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5027e-06, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pos_emb[0,1:]-pos_embeddings[0]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(patch_embeddings-patch_emb).abs().max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8c93ec7d301fbc7af32f2f0f6a4ce273ba8be80dd27f98a13b93c30d55f32a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
